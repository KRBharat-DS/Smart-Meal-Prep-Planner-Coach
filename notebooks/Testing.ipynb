{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d8afb3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import successful!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Add the 'src' directory to the Python path so we can import 'meal_planner'\n",
    "# This assumes you started Jupyter Lab from the project root ('smart-meal-planner')\n",
    "module_path = os.path.abspath(os.path.join('..', 'src'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "# Now import the function from our module\n",
    "from meal_planner.user_input import load_user_profile\n",
    "\n",
    "print(\"Import successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c77cdc28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load profile from: c:\\Users\\bhara\\Desktop\\Meal_Planner\\data\\examples\\user_profile_1.json\n",
      "Successfully loaded user profile from: c:\\Users\\bhara\\Desktop\\Meal_Planner\\data\\examples\\user_profile_1.json\n",
      "\n",
      "--- Loaded User Profile ---\n",
      "{'user_id': 'user123', 'fitness_goal': 'weight loss', 'dietary_preferences': ['vegetarian'], 'allergies_or_avoidances': ['nuts', 'shellfish'], 'taste_preferences': ['likes spicy', 'prefers savory breakfast'], 'schedule_constraints': ['dinner out Wednesday', 'needs quick lunches (under 30 mins prep+cook)'], 'specific_requests': ['Include pasta at least twice', 'No mushrooms']}\n",
      "\n",
      "Type of loaded data: <class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the example user profile, relative to the project root\n",
    "profile_file_path = os.path.join('..', 'data', 'examples', 'user_profile_1.json')\n",
    "# Convert to absolute path for clarity in printing\n",
    "abs_profile_path = os.path.abspath(profile_file_path)\n",
    "print(f\"Attempting to load profile from: {abs_profile_path}\")\n",
    "\n",
    "# Call the function to load the profile\n",
    "user_profile = load_user_profile(abs_profile_path) # Use absolute path here\n",
    "\n",
    "# Check if loading was successful and print the result\n",
    "if user_profile:\n",
    "    print(\"\\n--- Loaded User Profile ---\")\n",
    "    print(user_profile)\n",
    "    print(f\"\\nType of loaded data: {type(user_profile)}\")\n",
    "else:\n",
    "    print(\"\\nFailed to load user profile.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a866ac17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.exists(r\"C:\\Users\\bhara\\Desktop\\Meal_Planner\\data\\examples\\user_profile_1.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f5bc544f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\bhara\\\\Desktop\\\\Meal_Planner\\\\data\\\\examples\\\\user_profile_1.json'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs_profile_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f513b57f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb72e22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "baebff16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n",
    "import json # Import json for parsing potential JSON output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fe2e4271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Configure the generative AI client\n",
    "api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"GEMINI_API_KEY not found in .env file or environment variables.\")\n",
    "\n",
    "genai.configure(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "637a18d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Configuration ---\n",
    "# Choose the model appropriate for text generation and potentially JSON output\n",
    "# 'gemini-pro' is a good starting point.\n",
    "# Models supporting explicit JSON mode might be named differently (e.g., check Gemini docs)\n",
    "MODEL_NAME = \"gemini-1.5-pro-latest\" # Or \"gemini-pro\" if 1.5 isn't available/needed yet\n",
    "\n",
    "# Define generation configuration for potentially forcing JSON output\n",
    "# Note: Explicit JSON mode might require specific model versions or settings.\n",
    "# Refer to Google AI documentation for the latest on enforcing JSON.\n",
    "# This is a general approach; specific JSON mode might use different parameters.\n",
    "GENERATION_CONFIG_JSON = {\n",
    "  \"temperature\": 0.5, # Adjust creativity (lower is more deterministic)\n",
    "  \"top_p\": 0.95,\n",
    "  \"top_k\": 40,\n",
    "  \"max_output_tokens\": 2048, # Adjust as needed for plan length\n",
    "  \"response_mime_type\": \"application/json\", # Request JSON output\n",
    "}\n",
    "\n",
    "# --- Client Function ---\n",
    "\n",
    "def generate_text_with_retry(prompt_text, generation_config=None, safety_settings=None, retries=2):\n",
    "    \"\"\"\n",
    "    Calls the Gemini API to generate text based on a prompt, with retries.\n",
    "\n",
    "    Args:\n",
    "        prompt_text (str): The input prompt for the model.\n",
    "        generation_config (dict, optional): Configuration for generation (temp, tokens, etc.).\n",
    "                                            Defaults to a basic config if None.\n",
    "        safety_settings (list, optional): Configuration for safety filters. Defaults to None.\n",
    "        retries (int): Number of times to retry on specific errors.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated text content from the model, or None if generation fails after retries.\n",
    "    \"\"\"\n",
    "    if generation_config is None:\n",
    "        # Default config if none provided (might not force JSON here)\n",
    "        generation_config = {\n",
    "            \"temperature\": 0.7,\n",
    "            \"top_p\": 0.95,\n",
    "            \"top_k\": 40,\n",
    "            \"max_output_tokens\": 2048,\n",
    "        }\n",
    "\n",
    "    model = genai.GenerativeModel(\n",
    "        model_name=MODEL_NAME,\n",
    "        generation_config=generation_config,\n",
    "        safety_settings=safety_settings\n",
    "        # system_instruction=\"You are a helpful meal planning assistant.\" # Optional: Set system-level instructions\n",
    "    )\n",
    "\n",
    "    attempt = 0\n",
    "    while attempt <= retries:\n",
    "        try:\n",
    "            print(f\"\\n--- Calling Gemini API (Attempt {attempt + 1}/{retries + 1}) ---\")\n",
    "            response = model.generate_content(prompt_text)\n",
    "            print(\"--- Gemini API Call Successful ---\")\n",
    "\n",
    "            # Basic check if response has text content\n",
    "            if response.parts:\n",
    "                 # Accessing the text directly, assuming it's the first part\n",
    "                 # If using JSON mode, this should ideally be the JSON string\n",
    "                generated_content = response.text\n",
    "                # print(f\"Raw Response Text:\\n{generated_content[:500]}...\") # Optional: Print start of raw response\n",
    "                return generated_content\n",
    "            else:\n",
    "                 # Handle cases where the response might be blocked or empty\n",
    "                 print(\"Warning: Response received but contains no usable parts.\")\n",
    "                 # Check for blocking reasons if available\n",
    "                 if response.prompt_feedback and response.prompt_feedback.block_reason:\n",
    "                     print(f\"Prompt blocked due to: {response.prompt_feedback.block_reason}\")\n",
    "                 # Check candidates for finish reasons\n",
    "                 if response.candidates and response.candidates[0].finish_reason != 'STOP':\n",
    "                     print(f\"Generation stopped due to: {response.candidates[0].finish_reason}\")\n",
    "                 return None # Indicate failure or empty response\n",
    "\n",
    "        except Exception as e:\n",
    "            # Catching a broad range of potential API errors\n",
    "            print(f\"Error during Gemini API call (Attempt {attempt + 1}): {e}\")\n",
    "            attempt += 1\n",
    "            if attempt > retries:\n",
    "                print(\"Max retries reached. Generation failed.\")\n",
    "                return None\n",
    "            print(\"Retrying...\")\n",
    "            # Consider adding a small delay here before retrying: time.sleep(1)\n",
    "\n",
    "    return None # Should not be reached if loop logic is correct, but acts as fallback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2d648596",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Helper to Parse JSON ---\n",
    "def parse_json_output(text_output):\n",
    "    \"\"\"\n",
    "    Attempts to parse a string assumed to contain JSON into a Python dictionary.\n",
    "\n",
    "    Args:\n",
    "        text_output (str): The string output from the LLM.\n",
    "\n",
    "    Returns:\n",
    "        dict: The parsed dictionary, or None if parsing fails or input is None.\n",
    "    \"\"\"\n",
    "    if not text_output:\n",
    "        print(\"Error: No text output received from LLM to parse.\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        # Sometimes the LLM might wrap the JSON in markdown ```json ... ```\n",
    "        if text_output.strip().startswith(\"```json\"):\n",
    "            print(\"Detected JSON wrapped in markdown, attempting to extract.\")\n",
    "            # Extract content between the first ```json and the last ```\n",
    "            json_str = text_output.split(\"```json\", 1)[1].rsplit(\"```\", 1)[0].strip()\n",
    "        elif text_output.strip().startswith(\"{\") and text_output.strip().endswith(\"}\"):\n",
    "             # Assume it's already a JSON string if it starts/ends with braces\n",
    "             json_str = text_output.strip()\n",
    "        else:\n",
    "            # If it doesn't look like JSON, maybe the LLM failed the instruction\n",
    "            print(\"Warning: Output doesn't look like JSON. Attempting direct parse anyway.\")\n",
    "            json_str = text_output.strip() # Try parsing directly just in case\n",
    "\n",
    "        parsed_json = json.loads(json_str)\n",
    "        print(\"Successfully parsed JSON output.\")\n",
    "        return parsed_json\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error: Failed to decode JSON from LLM output. Error: {e}\")\n",
    "        print(f\"Problematic Text (first 500 chars):\\n{text_output[:500]}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during JSON parsing: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8d0fb232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing LLM Client...\n",
      "Sending test prompt: 'Explain the concept of a virtual environment in Python in one sentence.'\n",
      "\n",
      "--- Calling Gemini API (Attempt 1/3) ---\n",
      "--- Gemini API Call Successful ---\n",
      "\n",
      "--- Test Response ---\n",
      "A virtual environment is an isolated Python environment that allows you to install packages and dependencies for a specific project without affecting your global Python installation or other projects.\n",
      "\n",
      "\n",
      "--- Testing JSON Parsing ---\n",
      "\n",
      "Testing good JSON string:\n",
      "Detected JSON wrapped in markdown, attempting to extract.\n",
      "Successfully parsed JSON output.\n",
      "{'day_1': {'breakfast': 'Oatmeal', 'lunch': 'Salad', 'dinner': 'Chicken'}}\n",
      "\n",
      "Testing bad JSON string:\n",
      "Warning: Output doesn't look like JSON. Attempting direct parse anyway.\n",
      "Error: Failed to decode JSON from LLM output. Error: Expecting value: line 1 column 1 (char 0)\n",
      "Problematic Text (first 500 chars):\n",
      "Here is the plan: { \"day_1\": \"bad json\" \n",
      "\n",
      "Testing None input:\n",
      "Error: No text output received from LLM to parse.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example Usage (Optional - for testing this file directly)\n",
    "if __name__ == '__main__':\n",
    "    print(\"Testing LLM Client...\")\n",
    "    # Simple test prompt - NOT asking for JSON here, just testing connection\n",
    "    test_prompt = \"Explain the concept of a virtual environment in Python in one sentence.\"\n",
    "    print(f\"Sending test prompt: '{test_prompt}'\")\n",
    "    result = generate_text_with_retry(test_prompt)\n",
    "\n",
    "    if result:\n",
    "        print(\"\\n--- Test Response ---\")\n",
    "        print(result)\n",
    "    else:\n",
    "        print(\"\\n--- Test Failed ---\")\n",
    "\n",
    "    # Test JSON parsing helper\n",
    "    print(\"\\n--- Testing JSON Parsing ---\")\n",
    "    good_json_string = '```json\\n{\\n  \"day_1\": {\\n    \"breakfast\": \"Oatmeal\",\\n    \"lunch\": \"Salad\",\\n    \"dinner\": \"Chicken\"\\n  }\\n}\\n```'\n",
    "    bad_json_string = 'Here is the plan: { \"day_1\": \"bad json\" '\n",
    "    none_input = None\n",
    "\n",
    "    print(\"\\nTesting good JSON string:\")\n",
    "    parsed = parse_json_output(good_json_string)\n",
    "    if parsed: print(parsed)\n",
    "\n",
    "    print(\"\\nTesting bad JSON string:\")\n",
    "    parsed = parse_json_output(bad_json_string)\n",
    "    if parsed: print(parsed) # Should fail\n",
    "\n",
    "    print(\"\\nTesting None input:\")\n",
    "    parsed = parse_json_output(none_input)\n",
    "    if parsed: print(parsed) # Should fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfaee9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184ab5cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2467db1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "24faeba3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mjson\u001b[39;00m \u001b[38;5;66;03m# For potentially validating/handling JSON\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Import functions from our other modules within the same package\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllm_client\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m generate_text_with_retry, parse_json_output, GENERATION_CONFIG_JSON\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01muser_input\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_user_profile\n",
      "\u001b[1;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import yaml # For loading the prompt from YAML\n",
    "import json # For potentially validating/handling JSON\n",
    "\n",
    "# Import functions from our other modules within the same package\n",
    "from .llm_client import generate_text_with_retry, parse_json_output, GENERATION_CONFIG_JSON\n",
    "from .user_input import load_user_profile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2de8a60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mealprep_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
